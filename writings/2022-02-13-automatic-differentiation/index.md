---
title: auto grad
created: 2020-02-13
tags:
  - machine-learning
  - compiler
draft: true
---

做成一个系列???

自动微分的相关技术, 参考实现

- pytorch autograd
- jax autograd
- taichi

differentiable programming swift? julia?

常见的梯度求解方法包括: 数值微分（Numerical Differentiation）、符号微分
（Symbolic Differentiation）和自动微分（Automatic Differentiation）。

- https://zhuanlan.zhihu.com/p/69294347
- https://arxiv.org/abs/1502.05767
- https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/
- https://mp.weixin.qq.com/s/COXJW0mj-mHAQxIUOIvreg
- Wow: https://thenumb.at/Autodiff/

- [google/autobound](https://github.com/google/autobound): AutoBound
  automatically computes upper and lower bounds on functions.
- https://twitter.com/kmett/status/1647254134800752641
- https://ai.googleblog.com/2023/04/beyond-automatic-differentiation.html
- https://lotabout.me/2023/Auto-Differentiation-Part-1-Algorithm/
- https://lotabout.me/2023/Auto-Differentiation-Part-2-Implementation/
- https://lotabout.me/2023/Matrix-Operator-in-Neural-Network/
- https://vmartin.fr/understanding-automatic-differentiation-in-30-lines-of-python.html
